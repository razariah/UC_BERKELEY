{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1 -- Instructor Turn - Pyspark DataFrame Basic  👩‍🏫🧑‍🏫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://s3.amazonaws.com/dataviz-curriculum/day_1/food.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"food.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print our schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe our data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import struct fields that we can use\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need to create the list of struct fields\n",
    "schema = [StructField(\"food\", StringType(), True), StructField(\"price\", IntegerType(), True),]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in our fields\n",
    "final = StructType(fields=schema)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data with our new schema\n",
    "dataframe = spark.read.csv(SparkFiles.get(\"food.csv\"), schema=final, sep=\",\", header=True)\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print it out\n",
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataframe['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.select('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataframe.select('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.select('price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column\n",
    "dataframe.withColumn('newprice', dataframe['price']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update column name\n",
    "dataframe.withColumnRenamed('price','newerprice').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double the price\n",
    "dataframe.withColumn('doubleprice',dataframe['price']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dollar to the price\n",
    "dataframe.withColumn('add_one_dollar',dataframe['price']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half the price\n",
    "dataframe.withColumn('half_price',dataframe['price']/2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting a column as a list\n",
    "dataframe.select(\"price\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting PySpark DataFrame to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = dataframe.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2 -- Student Turn - Pyspark DataFrame Basic  👩‍🎓👨‍🎓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://s3.amazonaws.com/dataviz-curriculum/day_1/demographics.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"demographics.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first 10 rows\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the age, height_meter, and weight_kg columns and use describe to show the summary statistics\n",
    "df.select([\"age\", \"height_meter\", \"weight_kg\"]).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema to see the types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the Salary column to `Salary (1k)` and show only this new column\n",
    "df = df.withColumnRenamed('Salary', 'Salary (1k)')\n",
    "df.select(\"Salary (1k)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called `Salary` where the values are the `Salary (1k)` * 1000\n",
    "# Show the columns `Salary` and `Salary (1k)`\n",
    "df = df.withColumn(\"Salary\", df[\"Salary (1k)\"] * 1000)\n",
    "df.select([\"Salary\", \"Salary (1k)\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 -- Instructor Turn  - PySpark Filtering 👩‍🏫🧑‍🏫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkFiltering\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_1/wine.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"wine.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order a DataFrame by ascending values\n",
    "df.orderBy(df[\"points\"].asc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from pyspark.sql.functions import avg\n",
    "df.select(avg(\"points\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SQL\n",
    "df.filter(\"price<20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by price on certain columns\n",
    "df.filter(\"price<20\").select(['points','country', 'winery','price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python Comparison Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same results only this time using python\n",
    "df.filter(df[\"price\"] < 200).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter( (df[\"price\"] < 200) | (df['points'] > 80) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"country\"] == \"US\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4 -- Student Turn - Pyspark DataFrame Filtering 👩‍🎓👨‍🎓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkFiltering\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://s3.amazonaws.com/dataviz-curriculum/day_1/demographics.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"demographics.csv\"), inferSchema=True, sep=',')\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What occupation had the highest salary?\n",
    "df.orderBy(df[\"Salary\"].desc()).select(\"occupation\", \"Salary\").limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What occupation had the lowest salary?\n",
    "df.orderBy(df[\"Salary\"]).select(\"occupation\", \"Salary\").limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the mean salary of this dataset?\n",
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the max and min of the Salary column?\n",
    "from pyspark.sql.functions import max, min\n",
    "df.select(max(\"Salary\"), min(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all of the occupations where salaries were above 80k\n",
    "from pyspark.sql.functions import count\n",
    "df.filter(\"Salary > 80\").select(\"occupation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS\n",
    "# What is the average age and height for each academic degree type?\n",
    "# HINT: You will need to use `groupby` to solve this\n",
    "avg_df = df.groupBy(\"academic_degree\").avg()\n",
    "avg_df.select(\"academic_degree\", \"avg(age)\", \"avg(height_meter)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B1 -- Instructor Turn - Pyspark NLP Tokens 👩‍🏫🧑‍🏫\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"NLPTokens\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "dataframe = spark.createDataFrame([\n",
    "    (0, \"Spark is great\"),\n",
    "    (1, \"We are learning Spark\"),\n",
    "    (2, \"Spark is better than hadoop no doubt\")\n",
    "], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize word\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame\n",
    "dataframe.show()\n",
    "# Transform and show DataFrame\n",
    "tokenized = tokenizer.transform(dataframe)\n",
    "tokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2 -- Instructor Turn - UDF 👩‍🏫🧑‍🏫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"UDF\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.createDataFrame([\n",
    "    (0, \"Mary had a little lamb\"),\n",
    "    (1, \"It's fleece was white as snow\"),\n",
    "    (2, \"And everywhere Mary went\"),\n",
    "    (3, \"The lamb was sure to go\")\n",
    "], [\"id\", \"Nursery Rhyme\"])\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize word\n",
    "tokenizer = Tokenizer(inputCol=\"Nursery Rhyme\", outputCol=\"words\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to return the length of a list\n",
    "def word_list_length(word_list):\n",
    "    return len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user defined function \n",
    "count_tokens = udf(word_list_length, IntegerType())\n",
    "count_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform DataFrame\n",
    "tokenized = tokenizer.transform(dataframe)\n",
    "\n",
    "# Select the needed columns and don't truncate results\n",
    "tokenized.select(\"Nursery Rhyme\", \"words\")\\\n",
    "    .withColumn(\"tokens\", count_tokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B3 -- Student Turn - Pyspark NLP Tokens 👩‍🎓👨‍🎓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"tokenizing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/data.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"data.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize DataFrame\n",
    "tokened = Tokenizer(inputCol=\"Poem\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform DataFrame\n",
    "tokenized = tokened.transform(df)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Function to count vowels\n",
    "def vowel_counter(words):\n",
    "    vowel_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        for letter in word:\n",
    "            if letter in ('a', 'e', 'i', 'o', 'u'):\n",
    "                vowel_count += 1\n",
    "\n",
    "    return vowel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a user defined function\n",
    "count_vowels = udf(vowel_counter, IntegerType())\n",
    "count_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame with the udf\n",
    "tokenized.select(\"Poem\", \"words\")\\\n",
    "    .withColumn(\"vowels\", count_vowels(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B4 -- Instructor Turn - Pyspark NLP Stopwords 👩‍🏫🧑‍🏫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StopWords\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords library\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"Big\", \"data\", \"is\", \"super\", \"powerful\"]),\n",
    "    (1, [\"This\", \"is\", \"going\", \"to\", \"be\", \"epic\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Remover\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and show data\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B5 -- Student Turn - Pyspark NLP Stopwords 👩‍🎓👨‍🎓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StopWords\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/food_reviews.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"food_reviews.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize DataFrame\n",
    "review_data = Tokenizer(inputCol=\"Reviews\", outputCol=\"Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform DataFrame\n",
    "reviewed = review_data.transform(df)\n",
    "reviewed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"Words\", outputCol=\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform new DataFrame\n",
    "newFrame = remover.transform(reviewed)\n",
    "newFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show simplified review\n",
    "newFrame.select(\"filtered\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B6 -- Instructor Turn - Hashing TF 👩‍🏫🧑‍🏫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Hashing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with repeating words\n",
    "dataframe = spark.createDataFrame([\n",
    "    (0, \"The cow cow jumped and jumped cow\"),\n",
    "    (1, \"then the cow said\"),\n",
    "    (2, \"I am a cow that jumped\")\n",
    "],[\"id\", \"words\"])\n",
    "\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "tokenizer = Tokenizer(inputCol=\"words\", outputCol=\"tokens\")\n",
    "wordsData = tokenizer.transform(dataframe)\n",
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hashing term frequency\n",
    "hashing = HashingTF(inputCol=\"tokens\", outputCol=\"hashedValues\", numFeatures=pow(2,4))\n",
    "\n",
    "# Transform into a DF\n",
    "hashed_df = hashing.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display new DataFrame\n",
    "hashed_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the IDF on the data set \n",
    "idf = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
    "idfModel = idf.fit(hashed_df)\n",
    "rescaledData = idfModel.transform(hashed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "rescaledData.select(\"words\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B7 -- Student Turn - Hashing TF 👩‍🎓👨‍🎓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Hashing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/airlines.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"airlines.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize DataFrame\n",
    "tokened = Tokenizer(inputCol=\"Airline Tweets\", outputCol=\"words\")\n",
    "tokened_transformed = tokened.transform(df)\n",
    "tokened_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_list = [\"@VirginAmerica\", \"$30\", \"@virginamerica\"]\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stop_list)\n",
    "removed_frame = remover.transform(tokened_transformed)\n",
    "removed_frame.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hashing term frequency\n",
    "hashing = HashingTF(inputCol=\"filtered\", outputCol=\"hashedValues\", numFeatures=pow(2,4))\n",
    "\n",
    "# Transform into a DF\n",
    "hashed_df = hashing.transform(removed_frame)\n",
    "hashed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the IDF on the data set \n",
    "idf = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
    "idfModel = idf.fit(hashed_df)\n",
    "rescaledData = idfModel.transform(hashed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "rescaledData.select(\"words\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1 -- Everyone Turn - S3 👩‍🏫🧑‍🏫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Files:**\n",
    "\n",
    "  * [AWS Free Tier](Supplemental/AWS-Free-Tier.pdf)\n",
    "\n",
    "  * [AWS_RDS_guide.pdf](Supplemental/AWS_RDS_guide.pdf)\n",
    "\n",
    "* **Important!** [AWS Free Tier](Supplemental/AWS-Free-Tier.pdf) Review this documentation in order to avoid accidentally incurring charges.\n",
    "\n",
    "* Students can follow this activity along with a PDF guide. AWS_RDS_guide.pdf\n",
    "\n",
    "* [AWS Free Tier](https://aws.amazon.com/free/) and ask students to create a Free Tier account.\n",
    "\n",
    "* Amazon Web Services. Everything used in class will be available under Amazon's Free Tier program, but students should be careful not to choose any options that have a cost associated with it. Students should also delete their RDS databases after class so that no further costs are incurred. We will cover the steps for deleting RDS databases at the end of class.\n",
    "\n",
    "* Log in to the AWS Management Console and navigate to the **RDS** section under **Database**.\n",
    "\n",
    "  ![rds_console](Images/rds_console.png)\n",
    "\n",
    "* Click **Create database** from the **Create database** section to the right. This button will take you to the **Engine options** page, which brings up a menu of different relational databases. **Note** AWS may have a different screen than the one pictured below. If this is the first time using the service, the orange **Create database** will still be on the right.\n",
    "\n",
    "  ![create_db_button](Images/create_db_button.png)\n",
    "\n",
    "  **Note**: There may be an option to create a database with Amazon Aurora, which is a paid database. We will not be using this in today's lesson.\n",
    "\n",
    "* Check the box next to **Only enable options eligible for RDS Free Usage Tier** at the bottom of the menu.\n",
    "\n",
    "* Select **PostgreSQL** and click **Next**.\n",
    "\n",
    "  ![postgres_select](Images/postgres_select.png)\n",
    "\n",
    "* Keep the default settings for the instance specifications.\n",
    "\n",
    "* Fill out the fields under Settings. Use **myPostgresDB** as the database instance identifier and **root** as the master username.\n",
    "\n",
    "  **Note**: While the database instance identifier and master username can be anything, we recommend sticking to these settings in this case for consistency.\n",
    "\n",
    "* Enter a password and be sure to record it somewhere. The other settings will be accessible in the future, but the password will not. Then click **Next** to continue.\n",
    "\n",
    "  ![db settings](Images/db_settings.png)\n",
    "\n",
    "* Under **Network & Security** select **Yes** under the **Public accessibility** option. Explain that this does not mean anyone can access the database since a password is still needed, but allows connections from outside sources like pgAdmin.\n",
    "\n",
    "  ![public accessible](Images/public_accessible.png)\n",
    "\n",
    "* Under **Database Options**, make the database name **my_data_class_db**. (Use this name for the sake of consistency. In the future, any name can be used.) Keep the default settings in the other fields.\n",
    "\n",
    "  ![database_options](Images/database_options.png)\n",
    "\n",
    "* Click **Create Database** followed by **View DB Instance details** to navigate to the instance console page. The database creation on AWS's end will take anywhere from 10 to 15 minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2 -- Everyone Turn - S3 RDS 👩‍🎓👨‍🎓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Intro \n",
    "\n",
    "  * Simple Storage Service, or S3, is Amazon's cloud file storage service that uses key-value pairs. Files are stored on multiple servers and have a high rate of availability.\n",
    "\n",
    "  * S3 uses *buckets* to store files, which are similar to computer folders or directories. Buckets can contain additional folders and files. Each bucket must have a unique name.\n",
    "\n",
    "  * S3 has fine-grained control over files, such as read and write permissions. Buckets can assign individual access or total public access.\n",
    "\n",
    "* **Files:**\n",
    "\n",
    "  * [dog.png](Activities/C1-Evr_S3/Resources/dog.png)\n",
    "\n",
    "  * [S3_guide.pdf](Supplemental/S3_guide.pdf)\n",
    "\n",
    "\n",
    "\n",
    "* Explain the following points:\n",
    "\n",
    "  * AWS's S3 is a cloud-based file storage service.\n",
    "\n",
    "  * Files are stored on multiple servers, providing redundancy for data.\n",
    "\n",
    "  * Amazon guarantees an uptime, or availability, of over 99.99% for S3 files.\n",
    "\n",
    "  * On S3, files are organized by buckets.\n",
    "\n",
    "  * The S3 bucket structure is somewhat similar to a GitHub repository, which also holds files and folders.\n",
    "\n",
    "  * Each S3 bucket must have a URL that is unique across AWS.\n",
    "\n",
    "  * An S3 bucket can contain files, but it cannot contain another bucket.\n",
    "\n",
    "  * In this case, the region precedes `amazonaws.com`, followed by the bucket name and the filename.\n",
    "\n",
    "    ![Images/s300.png](Images/s300.png)\n",
    "\n",
    "  * S3 provides a high level of control over the files. At both the bucket and file levels, it is possible to control read and write access to different individuals and organizations.\n",
    "\n",
    "* Tell students to follow along for the rest of the activity.\n",
    "\n",
    "  * Go to console.aws.amazon.com and select S3 under Storage.\n",
    "\n",
    "    ![s3 console](Images/s3_console.png)\n",
    "\n",
    "  * Click **Create bucket**.\n",
    "\n",
    "    ![click create](Images/create_bucket.png)\n",
    "\n",
    "  * Create a bucket name and choose the region.\n",
    "\n",
    "  * **Note:** The bucket name must be unique across all existing bucket names in Amazon S3. Buckets cannot be renamed or created inside of another bucket.\n",
    "\n",
    "  * Leave the region as the default `US East (N. Virginia)`. Changing the region will change the object Url used in all examples today.\n",
    "\n",
    "    ![Images/s301.png](Images/s301.png)\n",
    "\n",
    "  * Most of the options on the **Configure Options** tab can be left as the default values.\n",
    "\n",
    "  * Tags are user-defined key-value pairs of information that can help keep track of buckets.\n",
    "\n",
    "  * Click **Next**.\n",
    "\n",
    "    ![Images/s302.png](Images/s302.png)\n",
    "\n",
    "  * The **Set Permissions** page is where we grant others permission to access buckets.\n",
    "\n",
    "    * A number of [security breaches](https://securityboulevard.com/2018/01/leaky-buckets-10-worst-amazon-s3-breaches/) were caused by unsecured S3 buckets.\n",
    "    * Public access is denied by default.\n",
    "\n",
    "  * Leave the boxes checked and click **Next**.\n",
    "\n",
    "    ![Images/s303.png](Images/s303.png)\n",
    "\n",
    "  * The **Review** page is a summary of the bucket configurations. Click **Create bucket**. The bucket name now appears in the S3 console.\n",
    "\n",
    "    ![Images/s304.png](Images/s304.png)\n",
    "\n",
    "    ![Images/s305.png](Images/s305.png)\n",
    "\n",
    "  * Explain that we'll now upload a file to the newly created bucket. Click the bucket name and then click **Upload**.\n",
    "\n",
    "  * A file can be dragged to the screen. Demonstrate by uploading [dog.png](Activities/C1-Evr_S3/Resources/dog.png) into the S3 bucket.\n",
    "\n",
    "  * Click **Upload**.\n",
    "\n",
    "    ![Images/s315.png](Images/s315.png)\n",
    "\n",
    "    ![Images/s316.png](Images/s316.png)\n",
    "\n",
    "  * Click the filename.\n",
    "\n",
    "    ![Images/s317.png](Images/s317.png)\n",
    "\n",
    "  * Explain why clicking the link leads to an error message.\n",
    "\n",
    "    ![Images/s308.png](Images/s308.png)\n",
    "\n",
    "    ![Images/s309.png](Images/s309.png)\n",
    "\n",
    "  * By default, the permission for the file denies access to everyone, so it needs to be changed.\n",
    "\n",
    "  * Navigate back to the dashboard by clicking **Amazon S3** on the top left.\n",
    "\n",
    "    ![Images/s3_dashboard](Images/s3_dashboard.png)\n",
    "\n",
    "  * Check the box next to your bucket and click **Edit public access settings**.\n",
    "\n",
    "    ![Images/edit_public.png](Images/edit_public.png)\n",
    "\n",
    "  * Make sure all boxes are unchecked on the next screen. Even though these were checked in the initial setup, they will not be now.\n",
    "\n",
    "    ![Images/bucket_public.png](Images/bucket_policy.png)\n",
    "\n",
    "  * Click **Save**. Then type **confirm** and click **Confirm**.\n",
    "\n",
    "    ![Images/confirm_policy.png](Images/confirm_policy.png)\n",
    "\n",
    "  * Next, navigate back into your bucket and check the box next to the image. Click the **Actions** box on the top and select **Make public**.\n",
    "\n",
    "    ![Images/bucket_public.png](Images/bucket_public.png)\n",
    "\n",
    "  * Now the image will be displayed when you click on the link.\n",
    "\n",
    "* Tell students that they can explore various settings at the bucket level and the file level. Use the tabs at the bucket level to illustrate the available settings, such as tags:\n",
    "\n",
    "  ![Images/s306.png](Images/s306.png)\n",
    "\n",
    "* **Note:** Students can remove public access anytime by repeating the steps above and checking all the boxes in **Edit public access settings**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3 -- Instructor Turn - ETL S3 RDS 👩‍🏫🧑‍🏫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First make sure that everyone has a database to use. Database creation was initiated at the beginning of class. Students whose databases are not yet running should follow along with a partner until their database is available.\n",
    "\n",
    "* Explain the following about the new RDS database:\n",
    "\n",
    "  * RDS stands for Relational Database Service. This is what Amazon uses to host a variety of relational databases in the cloud.\n",
    "\n",
    "  * These databases can have different dialects, such as MySQL, PostgreSQL, and Amazon's own Aurora database.\n",
    "\n",
    "  * The database that was created at the beginning of class uses PostgreSQL.\n",
    "\n",
    "* Navigate to the DB instance in the console created earlier. There will be a lot of information available, but we'll use only a few points of interest. Go over the console page, explaining these key points:\n",
    "\n",
    "  * The **Summary** section shows the kind of database the instance is and whether it is available.\n",
    "\n",
    "    ![db summary](Images/db_summary.png)\n",
    "\n",
    "  * The database metrics can largely be ignored for now.\n",
    "\n",
    "  * The **Connectivity** tab lists the endpoint, port, and security groups associated with the instance. The endpoint will be used to connect to the database.\n",
    "\n",
    "    ![db connection](Images/db_connection.png)\n",
    "\n",
    "  * The rest of the tabs contain more information about the instance, such as backups and logs, but students will not need to be concerned with this for class.\n",
    "\n",
    "# RDS \n",
    "\n",
    "* **File:**\n",
    "\n",
    "  * [RDS_pgAdmin_guide.pdf](Supplemental/RDS_pgAdmin_guide.pdf)\n",
    "\n",
    "* Slack out the PDF guide, which students can use to follow along.\n",
    "\n",
    "* Make sure everyone has the pgAdmin 4 UI installed. Direct students who do not have it installed to the [pgAdmin download page](https://www.pgadmin.org/download/) to download the appropriate version for their operating system.\n",
    "\n",
    "* Open up the pgAdmin UI. Explain the following to students:\n",
    "\n",
    "  * pgAdmin can connect to a cloud-based database, such as AWS, as well as local databases.\n",
    "\n",
    "  * pgAdmin offers a visual interface for managing data.\n",
    "\n",
    "* Log in to the AWS console and navigate to **RDS** under **Database**.\n",
    "\n",
    "  ![RDS console](Images/rds_console.png)\n",
    "\n",
    "* Navigate to **Instances** in the **Resources** section to the right.\n",
    "\n",
    "  ![instance_menu.png](Images/instance_menu.png)\n",
    "\n",
    "* Go to the database created earlier, `mypostgresdb`.\n",
    "\n",
    "* Navigate to the **Security Group** rules section on the right and explain the following:\n",
    "\n",
    "  * These security groups tell the RDS instance what traffic is allowed into and out of the database.\n",
    "\n",
    "  * The security settings can range from restrictive to open.\n",
    "\n",
    "  * In this activity, the database will be open to all traffic; however, this is not recommended for production code.\n",
    "\n",
    "* Click the security group for type **CIDR/IP - Inbound**.\n",
    "\n",
    "  ![security_inbound](Images/security_indbound.png)\n",
    "\n",
    "* This will navigate to a new page. Follow these steps to give the database access to all inbound traffic:\n",
    "\n",
    "  * From the management console, navigate to the Inbound tab on the bottom part of the screen, and then click **Edit**. This will bring up a menu to set rules for the security group.\n",
    "\n",
    "    ![inbound_edit](Images/inbound_edit.png)\n",
    "\n",
    "  * Change the Source to **Anywhere** and click **Save**. The RDS instance will now accept a connection from anywhere. This isn't completely open to the world because the endpoint, username, and password are still needed to connect.\n",
    "\n",
    "      ![ip_source](Images/ip_source.png)\n",
    "\n",
    "* Navigate back to the instance console and have the class find the endpoint, which is found in the **Connectivity** tab.\n",
    "\n",
    "  ![db connection](Images/db_connection.png))\n",
    "\n",
    "* Open up pgAdmin, right-click on **Servers**, and then go to **Create - Server**. Then walk through the following steps to create a connection to the AWS RDS instance:\n",
    "\n",
    "  * Under the **General** tab, enter the server name as **my_aws_postgres_rds**.\n",
    "\n",
    "    ![server name](Images/general_tab.png)\n",
    "\n",
    "  * Under the **Connection** tab, do the following:\n",
    "\n",
    "    * Enter the Endpoint in the **Hostname/address** field. This is unique to the instance.\n",
    "\n",
    "    * Enter 'postgres' in the **Maintenance database** field. This is the default for all postgres RDS instances.\n",
    "\n",
    "    * Enter the Username in the **Username** field, which is `root` in this case.\n",
    "\n",
    "    * Enter the password that was created for your RDS instance.\n",
    "\n",
    "    * Check the box next to **Save Password**.\n",
    "\n",
    "  * Click **Save**. If all information is entered correctly, this will set up the connection and not return an error.\n",
    "\n",
    "    ![connection tab](Images/connection_tab.png).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
